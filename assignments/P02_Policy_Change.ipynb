{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Policy Change\n",
    "## Quantified Cognition\n",
    "### Psychology 5332\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name: *Your Name Here*\n",
    "# User ID: *Your ID Here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "Upon completion of this assignment, the student will demonstrate the ability to:\n",
    "\n",
    "1. Modify the policy of an RL model\n",
    "2. Test whether the new policy improves learning in various environments\n",
    "3. Evaluate why the policy did or did not work and propose alternative approaches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "- The goal of this assignment is to modify the *policy* of the successor representation model to generate different behaviors given the environment. You will then test whether this change improves learning in different environmental contexts. \n",
    "\n",
    "- You will perform this assignment by writing code in *this notebook* (***after making a copy and renaming it to have your userid in the title --- e.g., P02_Policy_Change_mst3k***).\n",
    "\n",
    "- ***When you are done, save this notebook as HTML (`File -> Download as -> HTML`) and upload it to the matching assignment on UVACollab.***\n",
    "\n",
    "## HINTS\n",
    "\n",
    "- Be sure to comment your code\n",
    "- I have provided cells with general instructions for what they should contain.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Change\n",
    "\n",
    "As discussed in class, both learning and behavior in an environment depend on the agent's policy for selecting actions given the current state. We also discussed the importance of the exploration vs. exploitation trade-off during learning. Whether you exploit currently preferred actions or explore novel actions also depends on the policy.\n",
    "\n",
    "Here, we will modify the policy of the successor representation (SR) reinforcement learning (RL) model we covered in class. Specifically, the current policy involves evaluating future visitions to other states given a current state and all possible actions, and calculating a Q value for each action taking into account the learned rewards (or punishments) at each state. Mathematically, that involves:\n",
    "\n",
    "$$Q = (M \\cdot s_i) \\cdot r,$$\n",
    "\n",
    "where $M$ is the successor representation matrix, $s_i$ is the current state, and $r$ is the vector of rewards associated with each state.\n",
    "\n",
    "What we'd like to do is encourage the agent to move away from their most recent states, thereby enhancing exploration of the environment. Luckily, the agent already keeps track of recent states in the eligibility trace (or context, in the parlance of the temporal context model). Your job is to use this vector (which is already passed into the policy function as `t0`) along with a new parameter (let's call this `revisit_penalty`) that can take on values between zero and one, to modify the rewards associated with each state such that recent states are penalized.\n",
    "\n",
    "Once you have updated the policy code, train the model in an environment that is not slippery (i.e., set `slippery = False`) and make note of how many training iterations it needs to learn to a criterion of 25 correct in a row. Does the model learn faster when you penalize more for revisiting recent states? Make sure to assess this in a handful of random environments by rerunning the code that generates a random map (see below), so that your assessment isn't biased by a single environment.\n",
    "\n",
    "Next, make the environment slippery (i.e., set `slippery = True`) and see if penalizing revisitation of recent states helps or hurts learning. Is the model able to learn better or worse on average? Even if it never reaches the criterion, is it solvinging more or less often with the penalty?\n",
    "\n",
    "In addition to the questions above, here are some questions to answer in your write-up:\n",
    "\n",
    "- Why do you think you observed what you did?\n",
    "- Can you think of a different policy that may work better in slippery environments?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to install more libraries\n",
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load matplotlib inline mode\n",
    "%matplotlib inline\n",
    "\n",
    "# import some useful libraries\n",
    "import numpy as np                # numerical analysis linear algebra\n",
    "import matplotlib.pyplot as plt   # plotting\n",
    "\n",
    "from IPython.display import display, clear_output, Image\n",
    "import time\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "#import gym\n",
    "#from gym.envs.toy_text import frozen_lake\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text import frozen_lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment defined from class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the environment\n",
    "size = 10\n",
    "p_frozen = 0.9\n",
    "slippery = False\n",
    "\n",
    "# generate a random map\n",
    "desc = frozen_lake.generate_random_map(size=size, p=p_frozen)\n",
    "env = frozen_lake.FrozenLakeEnv(desc=desc,\n",
    "                                is_slippery=slippery,\n",
    "                                render_mode='ansi'\n",
    "                               )\n",
    "\n",
    "## reset the environment and get the initial state\n",
    "observation, info = env.reset()\n",
    "display(print(env.render()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy and Model from class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "gamma = .95\n",
    "alpha = .5\n",
    "rho = .25\n",
    "tau = 10.0\n",
    "p_rand = 0.0\n",
    "hole_penalty = -1.0\n",
    "off_board_penalty = -0.0\n",
    "\n",
    "# set up our agent\n",
    "# pull out the number of actions and unique states\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n\n",
    "\n",
    "# create orthogonal state representations\n",
    "states = np.eye(n_states)\n",
    "\n",
    "# allocate for where we learn:\n",
    "# rewards associated with each state\n",
    "rewards = np.zeros(n_states)\n",
    "# states associated with each other (via SR)\n",
    "M = np.zeros((n_actions, n_states, n_states))\n",
    "\n",
    "# keep track of scores during learning\n",
    "scores = []\n",
    "\n",
    "# define a policy \n",
    "# !!!!! MODIFY THIS CODE TO UPDATE THE POLICY !!!!\n",
    "def pick_action(f0, t0, M, rewards, tau, p_rand=0.0):\n",
    "    # apply policy to pick action\n",
    "    if p_rand > 0.0 and np.random.rand() < p_rand:\n",
    "        # pick a random action\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        Q = np.dot(np.dot(M, f0), rewards)\n",
    "        #action = np.random.choice(np.where(Q==Q.max())[0])\n",
    "        #action = np.argmax(Q)\n",
    "        pQ = np.exp(Q*tau)/np.exp(Q*tau).sum()\n",
    "        action = np.argmax(np.random.rand() < np.cumsum(pQ))\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrials = 1000\n",
    "max_moves = 500\n",
    "max_corr = 25\n",
    "\n",
    "for r in progress_bar(range(ntrials)):\n",
    "    # reset for new attempt at recovering the frisbee\n",
    "    observation, info = env.reset()\n",
    "    last_obs = observation\n",
    "    f0 = states[observation]\n",
    "    t0 = states[observation]\n",
    "    \n",
    "    # set current annealing\n",
    "    cur_p_rand = p_rand*(1-((r+1)/ntrials))\n",
    "\n",
    "    for i in range(max_moves):\n",
    "        # pick an action\n",
    "        action = pick_action(f0, t0, M, rewards, tau, p_rand=cur_p_rand)  \n",
    "    \n",
    "        # observe new state\n",
    "        observation, reward, trunc, done, info = env.step(action)\n",
    "        \n",
    "        # turn the new state into a vector representation\n",
    "        f1 = states[observation]\n",
    "\n",
    "        # learn via successor representation\n",
    "        # prediction from previous state\n",
    "        p0 = np.dot(M[action], f0)\n",
    "        \n",
    "        # observed outcome, plus discounted future prediction\n",
    "        # when following that policy\n",
    "        f1_action = pick_action(f1, t0, M, rewards, tau, p_rand=cur_p_rand)\n",
    "        o1 = (f1 + gamma*(np.dot(M[f1_action], f1)))\n",
    "        \n",
    "        # update the association for that action\n",
    "        M[action] += alpha * np.outer((o1 - p0), t0)\n",
    "\n",
    "        # update context (eligibility trace)\n",
    "        #t1 = rho*t0 + (1-rho)*f1\n",
    "        t1 = np.clip(rho*t0 + f1, 0, 1)\n",
    "\n",
    "        # process the reward if any\n",
    "        if trunc and reward==0:\n",
    "            # get negative rewards for falling in a hole\n",
    "            reward = hole_penalty\n",
    "            \n",
    "        if last_obs == observation:\n",
    "            # action gave rise to no change in movement\n",
    "            reward = off_board_penalty\n",
    "            \n",
    "        #if reward == 0:\n",
    "        #    # punish going to a state and not getting anything for it\n",
    "        #    rewards[last_obs] -= .1\n",
    "\n",
    "        # update our representation of rewards/punishments at the observed state\n",
    "        rewards[observation] += alpha*(reward - rewards[observation])\n",
    "\n",
    "        # see if we're done\n",
    "        if trunc:\n",
    "            #print(\"Episode finished after {} timesteps with reward {}\".format(i+1, reward))\n",
    "            # save out our final reward/punishment\n",
    "            scores.append(reward)\n",
    "            break\n",
    "\n",
    "        # prepare for next iteration\n",
    "        f0 = f1\n",
    "        t0 = t1\n",
    "        last_obs = observation\n",
    "    \n",
    "    # if we ran out of time, say we fell in\n",
    "    if i==(max_moves-1):\n",
    "        scores.append(hole_penalty)\n",
    "        \n",
    "    if len(scores)>max_corr and np.mean(scores[-max_corr:])==1.0:\n",
    "        # we're consistently solving it, so quit\n",
    "        break\n",
    "\n",
    "# render the final state\n",
    "env.render()\n",
    "\n",
    "# plot a moving average of scores\n",
    "N=10\n",
    "plt.plot(np.convolve(scores, np.ones((N,))/N, mode='valid'))\n",
    "\n",
    "print(\"Mean final performance:\", np.mean(scores[-max_corr:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your short answer here to the questions listed above:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
