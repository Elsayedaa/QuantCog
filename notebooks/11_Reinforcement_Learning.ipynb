{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lesson 11: Reinforcement Learning\n",
    "\n",
    "## Intro to Quantified Cognition\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/compmem/QuantCog/blob/2022_Fall/notebooks/11_Reinforcement_Learning.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lesson plan\n",
    "\n",
    "- Introduce Machine Learning\n",
    "- Fundamentals of Reinforcement Learning\n",
    "- Successor Representation\n",
    "- Frozen Lake example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://www.wordstream.com/wp-content/uploads/2021/07/machine-learning1-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fundamentals of RL\n",
    "\n",
    "- Reinforcement learning (RL) can combine both Supervised and Unsupervised Learning.\n",
    "- The goal is for the *agent* to learn *actions* to take in an *environment* to maximize *reward*.\n",
    "\n",
    "### Policy\n",
    "\n",
    "Defines how the agent behaves in the environment. This is typically a mapping between perceived states of the world and possible actions. The policy is essentially the \"rules to live by\" for the agent.\n",
    "\n",
    "### Reward function\n",
    "\n",
    "The rewards (and punishments) define the goals of RL problem. The environment provides a reward signal to the agent at every time step. \n",
    "\n",
    "The policy can be adjusted to maximize rewards and the agent can influence the rewards by performing different actions in the environment to change the state or (where possible) alter the environment directly.\n",
    "\n",
    "### Value function\n",
    "\n",
    "This function estimates the predicted future reward for a given state or state--action pair. Learning this function is at the core of RL and is the information that guides the policy governing behavior in the environment.\n",
    "\n",
    "### Model of the environment (optional)\n",
    "\n",
    "Although not required for all RL algorithms, it is often useful for the agent to learn a model of the environment that estimates what agent *thinks* will happen when performing specific actions in a particular state. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Markov Decision Problem\n",
    "\n",
    "Most RL can be formalized as a Markov Decision Problem (MDP) that involves sensing the *states* and perfoming *actions* to achieve a *goal*. \n",
    "\n",
    "The key feature of an MDP is that the probability of ending up in some new state $s_{t+1}'$ given that you are in the current state $s_t$ and perform action $a_t$ only depends on the current state and not any previous states. That is to say, all of the history can be integrated into the current state $s_t.$\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*QF9pmAOS5-Dn1B7RUH1zUA.png\" \n",
    "     alt=\"gridworld\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximizing *future* reward\n",
    "\n",
    "The goal of an agent is usually to maximize *future* reward, not just immediate reward. Thus an optimal policy can often have the agent forgo an immediate reward in order to gain more more rewards in the future:\n",
    "\n",
    "<img src=\"https://www.sciencenews.org/wp-content/uploads/2018/06/062918_BB_marshmallow-test_feat.jpg\" \n",
    "     alt=\"marshmallows\" width=\"500\"/>\n",
    "\n",
    "To keep this problem tracktable, instead of estimating out to infinity, the agent can estimate the *discounted* future reward:\n",
    "\n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4}... = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calculating value\n",
    "\n",
    "Estimating value is at the core of all RL algorithms. Value functions ($v$) are dependent on the policy ($\\pi$) for estimating future reward for a given state ($s$):\n",
    "\n",
    "$$v_\\pi(s) = \\mathbb{E}_\\pi(R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4}... \\vert S_t=s)$$\n",
    "\n",
    "This estimates the discounted future value of a state given the current policy. To guide decisions for how to act, the agent must also estimate the value of taking a particular action in a given state, which can be learned via temporal difference learning and the Bellman Equation:\n",
    "\n",
    "$$Q_\\pi(s_t, a_t) = Q_\\pi(s_t, a_t) + \\alpha (R_t + \\gamma max(Q_\\pi(s_{t+1}, a_{t+1})) - Q_\\pi(s_t, a_t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploration vs. Exploitation\n",
    "\n",
    "During learning, the agent must adopt a policy that balances exploration of the environment with exploitation of current knowledge. Usually this is accomplished by biasing the agent to make more random decisions at the start of learning and gradually shift to exploiting learned values later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Successor Representation\n",
    "\n",
    "Hybrid of Model-based and Model-free RL developed by Peter Dayan (1993), where the agent learns a model of the world and, separately, the reward values at each state they've experienced. Specifically, the agent learns the expected discounted number of future visitations of all states given a current state and action:\n",
    "\n",
    "$$M = M + \\alpha(f_{i+1} + \\gamma Mf_{i+1} - Mf_i) f_i^T$$\n",
    "\n",
    "and you can add in an eligibility trace (context) to learn faster:\n",
    "\n",
    "$$M = M + \\alpha(f_{i+1} + \\gamma Mf_{i+1} - Mf_i) t_i^T$$\n",
    "\n",
    "where\n",
    "\n",
    "$$t_i = \\rho t_{i-1} + f_i $$\n",
    "\n",
    "Thus, it is possible for and agent to make predictions of future states given specific state--actions and combine that information with known rewards/punishments to guide the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frisbee on a frozen lake problem\n",
    "\n",
    "\"Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\"\n",
    "\n",
    "https://gymnasium.farama.org/environments/toy_text/frozen_lake/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## *ONLY* if on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to install more libraries\n",
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load modules of power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load matplotlib inline mode\n",
    "%matplotlib inline\n",
    "\n",
    "# import some useful libraries\n",
    "import numpy as np                # numerical analysis linear algebra\n",
    "import matplotlib.pyplot as plt   # plotting\n",
    "\n",
    "from IPython.display import display, clear_output, Image\n",
    "import time\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "#import gym\n",
    "#from gym.envs.toy_text import frozen_lake\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text import frozen_lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFFFF\n",
      "FFFFFFFFFF\n",
      "FFFFFFFFFF\n",
      "FFFFFFHHFF\n",
      "FFFFFFFFFF\n",
      "FHFFFFFFFH\n",
      "FFFFFFFHFF\n",
      "FFFFFFFFFF\n",
      "FFFFFFFFFF\n",
      "FFFFFFFFFG\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the environment\n",
    "size = 10\n",
    "p_frozen = 0.9\n",
    "slippery = False\n",
    "\n",
    "# generate a random map\n",
    "desc = frozen_lake.generate_random_map(size=size, p=p_frozen)\n",
    "env = frozen_lake.FrozenLakeEnv(desc=desc,\n",
    "                                is_slippery=slippery,\n",
    "                                render_mode='ansi'\n",
    "                               )\n",
    "\n",
    "## reset the environment and get the initial state\n",
    "observation, info = env.reset()\n",
    "display(print(env.render()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFFFF\n",
      "FFFFFFFFFF\n",
      "FFFFFFFFFF\n",
      "FFFFFFHHFF\n",
      "FFFFFFFFFF\n",
      "FHFFFFFFFH\n",
      "FFFFFFFHFF\n",
      "FFFFFFFFFF\n",
      "FFFFFFFFFF\n",
      "FFFFFFFFFG\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A=L, S=D, D=R, W=U, Q=Quit:q\n",
      "You gave up and fell in.\n"
     ]
    }
   ],
   "source": [
    "## reset the environment and get the initial state\n",
    "observation, info = env.reset()\n",
    "display(print(env.render()))\n",
    "\n",
    "# action mapping\n",
    "amap = {'a': 0, 's':1, 'd':2, 'w':3, 'q': -1}\n",
    "\n",
    "def get_action():\n",
    "    done = False\n",
    "    while not done:\n",
    "        char = input('A=L, S=D, D=R, W=U, Q=Quit:').lower()\n",
    "        if char not in amap:\n",
    "            continue\n",
    "        return int(amap[char])\n",
    "\n",
    "# loop for a max number of iterations\n",
    "for t in range(100):\n",
    "    # pick an action at random\n",
    "    #action = env.action_space.sample()\n",
    "    action = get_action()\n",
    "    if action == -1:\n",
    "        # we're quitting\n",
    "        print(\"You gave up and fell in.\")\n",
    "        break\n",
    "    \n",
    "    # take that action and observe the results\n",
    "    observation, reward, trunc, done, info = env.step(action)\n",
    "    \n",
    "    # draw the new state to the screen\n",
    "    clear_output(wait=True)\n",
    "    display(print(env.render()))\n",
    "    \n",
    "    # see if we're done (either by falling in a hole or reaching the goal)\n",
    "    if trunc:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "    time.sleep(.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# params\n",
    "gamma = .95\n",
    "alpha = .5\n",
    "rho = .25\n",
    "tau = 10.0\n",
    "p_rand = 0.0\n",
    "hole_penalty = -1.0\n",
    "off_board_penalty = -0.0\n",
    "\n",
    "# set up our agent\n",
    "# pull out the number of actions and unique states\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n\n",
    "\n",
    "# create orthogonal state representations\n",
    "states = np.eye(n_states)\n",
    "\n",
    "# allocate for where we learn:\n",
    "# rewards associated with each state\n",
    "rewards = np.zeros(n_states)\n",
    "# states associated with each other (via SR)\n",
    "M = np.zeros((n_actions, n_states, n_states))\n",
    "\n",
    "# keep track of scores during learning\n",
    "scores = []\n",
    "\n",
    "# define a policy\n",
    "def pick_action(f0, t0, M, rewards, tau, p_rand=0.0):\n",
    "    # apply policy to pick action\n",
    "    if p_rand > 0.0 and np.random.rand() < p_rand:\n",
    "        # pick a random action\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        Q = np.dot(np.dot(M, f0), rewards)\n",
    "        #action = np.random.choice(np.where(Q==Q.max())[0])\n",
    "        #action = np.argmax(Q)\n",
    "        pQ = np.exp(Q*tau)/np.exp(Q*tau).sum()\n",
    "        action = np.argmax(np.random.rand() < np.cumsum(pQ))\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='63' class='' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      6.30% [63/1000 00:01&lt;00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean final performance: 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ntrials = 1000\n",
    "max_moves = 500\n",
    "max_corr = 25\n",
    "\n",
    "for r in progress_bar(range(ntrials)):\n",
    "    # reset for new attempt at recovering the frisbee\n",
    "    observation, info = env.reset()\n",
    "    last_obs = observation\n",
    "    f0 = states[observation]\n",
    "    t0 = states[observation]\n",
    "    \n",
    "    # set current annealing\n",
    "    cur_p_rand = p_rand*(1-((r+1)/ntrials))\n",
    "\n",
    "    for i in range(max_moves):\n",
    "        # pick an action\n",
    "        action = pick_action(f0, t0, M, rewards, tau, p_rand=cur_p_rand)  \n",
    "    \n",
    "        # observe new state\n",
    "        observation, reward, trunc, done, info = env.step(action)\n",
    "        \n",
    "        # turn the new state into a vector representation\n",
    "        f1 = states[observation]\n",
    "\n",
    "        # learn via successor representation\n",
    "        # prediction from previous state\n",
    "        p0 = np.dot(M[action], f0)\n",
    "        \n",
    "        # observed outcome, plus discounted future prediction\n",
    "        # when following that policy\n",
    "        f1_action = pick_action(f1, t0, M, rewards, tau, p_rand=cur_p_rand)\n",
    "        o1 = (f1 + gamma*(np.dot(M[f1_action], f1)))\n",
    "        \n",
    "        # update the association for that action\n",
    "        M[action] += alpha * np.outer((o1 - p0), t0)\n",
    "\n",
    "        # update context (eligibility trace)\n",
    "        #t1 = rho*t0 + (1-rho)*f1\n",
    "        t1 = np.clip(rho*t0 + f1, 0, 1)\n",
    "\n",
    "        # process the reward if any\n",
    "        if trunc and reward==0:\n",
    "            # get negative rewards for falling in a hole\n",
    "            reward = hole_penalty\n",
    "            \n",
    "        if last_obs == observation:\n",
    "            # action gave rise to no change in movement\n",
    "            reward = off_board_penalty\n",
    "            \n",
    "        #if reward == 0:\n",
    "        #    # punish going to a state and not getting anything for it\n",
    "        #    rewards[last_obs] -= .1\n",
    "\n",
    "        # update our representation of rewards/punishments at the observed state\n",
    "        rewards[observation] += alpha*(reward - rewards[observation])\n",
    "\n",
    "        # see if we're done\n",
    "        if trunc:\n",
    "            #print(\"Episode finished after {} timesteps with reward {}\".format(i+1, reward))\n",
    "            # save out our final reward/punishment\n",
    "            scores.append(reward)\n",
    "            break\n",
    "\n",
    "        # prepare for next iteration\n",
    "        f0 = f1\n",
    "        t0 = t1\n",
    "        last_obs = observation\n",
    "    \n",
    "    # if we ran out of time, say we fell in\n",
    "    if i==(max_moves-1):\n",
    "        scores.append(hole_penalty)\n",
    "        \n",
    "    if len(scores)>max_corr and np.mean(scores[-max_corr:])==1.0:\n",
    "        # we're consistently solving it, so quit\n",
    "        break\n",
    "\n",
    "# render the final state\n",
    "env.render()\n",
    "\n",
    "# plot a moving average of scores\n",
    "N=10\n",
    "plt.plot(np.convolve(scores, np.ones((N,))/N, mode='valid'))\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Score (moving average)')\n",
    "\n",
    "print(\"Mean final performance:\", np.mean(scores[-max_corr:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LEFT', 0.21031761377140853) ('DOWN', 0.30972484967871095) ('RIGHT', 0.22622119285902786) ('UP', 0.2537363436908527)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArkAAAC+CAYAAAAxxBclAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUk0lEQVR4nO3dfbBcdXnA8e/TmyAJAUESawnUC61jRaZFzCCCfRnRDr5UnU6nYisdbaf5o6DI+FKsdbQddcbW8jJTRouotUJ1aqStVUd0Rh1fQGoS6PASHTEgBBK5QYMhGJLcPP1j9w5LyL138zv3nM397fczw4Q9u88+z+599uS5J2d/JzITSZIkqSa/NOoCJEmSpIXmkCtJkqTqOORKkiSpOg65kiRJqo5DriRJkqrjkCtJkqTqdDbkRsR5EfGDiLgrIi7tKOdJEfH1iNgUEXdExMUd5Z2IiFsi4gtd5OvnPDYi1kXE9/uv94Ud5Lyk/77eHhGfjogj2845Cl337qj6tp+70961b9tl77aaz95tifNC6znHpnc7GXIjYgK4CngZcCrwuog4tYPU+4C3ZuZzgLOACzvKezGwqYM8g64EvpyZvwH8Vtv5I2I18GZgTWaeBkwA57eZcxRG1Luj6lvovnft25bYu62zd1vgvNCJsendro7kngnclZmbM3MP8Bng1W0nzcytmbmx//876f0gV7eZMyJOBF4BXNNmngNyHgP8DvAxgMzck5k7Oki9BFgWEUuA5cADHeTsWue9O4q+he57175tnb3bXj57tz3OC+3mHKve7WrIXQ3cN3B7Cx3s+AZFxCTwPODmllNdAbwD2N9ynkGnAFPAJ/r/7HFNRBzVZsLMvB/4EHAvsBV4ODO/0mbOERlp73bYt9B979q37bJ322Pvtsd5oV1j1btdDblxkG2dXU84IlYAnwPekpk/bzHPK4EHM3NDWzlmsQQ4A/hwZj4P2AW0eh5TRBxH77frk4ETgKMi4vVt5hyRkfVuV33bzzWK3rVv22XvtsfebY/zQrvGqne7GnK3ACcN3D6Rjg5VR8RSeg17XWZe33K6c4BXRcQ99P6J5cURcW3LOaH3/m7JzJnfOtfRa+I2vQS4OzOnMnMvcD1wdss5R2Ekvdtx38Joete+bZe92x57tz3OC+0aq97tasj9HvCsiDg5Io6gd8Lx59tOGhFB77yTTZl5Wdv5MvOdmXliZk7Se41fy8zWf1vJzG3AfRHx7P6mc4E7W057L3BWRCzvv8/n0v3J813ovHe77lsYTe/at62zd9vLae+2x3mh3bxj1btLukiSmfsi4iLgBnrfqvt4Zt7RQepzgAuA2yLi1v62v8nML3WQu2tvAq7r7xQ2A29sM1lm3hwR64CN9L6VegtwdZs5R2FEvWvftmRc+hbs3Q7Yuy1wXujE2PRuZHZ2qoskSZLUCa94JkmSpOo45EqSJKk6DrmSJEmqjkOuJEmSqtP5kBsRa81pzsVoXN5fc9ZlXN7bUf08x+m1dm1c3ltztmcUR3JH8eE0Z105R2Vc3l9z1mVc3ttR/TzH6bV2bVzeW3O2xNMVJEmSVJ1W1slduXJlTk5OHvS+qakpVq1aNXvw1rLLOO+d46J/DwHHzxG7pSgj7J3nvqWFsfPZN8v2/cz9W0uTn/RsscnBLzTe1H4gM9t46lkti8hj5rj/F8CyWe57rDDnfD+TPcARC5xz/xD3z9ZHTXporrxt9dFcWuzd7Zk5x05u4a1cETk5y05u6hFYtWKO4FVPL0t624Oz3jU1DasmZg/90Z6ylLPt+6Ddfe70HPfN9XmZ77NWqo3eHcU+F2DlsZGTJxz8vqmfwarj5gj+YVnO7XM00k7g6DliHypLOWcP7WPuK3PNFVuad74eauMKCqOYF1q54tnk5CTr168vC/5A2Vuw7V1l6QDeVhj3k/KUjS7EXfoh290gZ5O/IEo0qbXUMfSurVjinsK4Ju9r4f690XtbOlg3ydtkBz8Kj8KPu845eTys/+vC4L/608KklxcmhD8sfIemijM2i/1pYVyTz1rXfT+KfS7A5Amw/lOFweeVhX10e2E+4N8K43aWp2RHg9jSvG39UtiGuXrX0xUkSZJUHYdcSZIkVWeoITcizouIH0TEXRFxadtFSZIkSU3MO+RGxARwFfAy4FTgdRFxatuFSZIkSaWGOZJ7JnBXZm7OzD3AZ4BXt1uWJEmSVG6YIXc1cN/A7S39bZIkSdJhaZgh92Brej1pCbWIWBsR6yNi/dRUk8VapO4M9u0vRl2MdAiesM99ZNTVSMN7Qu/+bNTVqGbDDLlbgJMGbp/IQZZ5zcyrM3NNZq6Z82IP0mFksG9nu9CDdDh6wj53ros9SIeZJ/TuXBd7kBoaZsj9HvCsiDg5Io6gt17+59stS5IkSSo37xXPMnNfRFwE3ABMAB/PzDtar0ySJEkqNNRlfTPzS8CXWq5FkiRJWhBe8UySJEnVcciVJElSdYY6XeFQ7dmwgXviYCuPze/+wpx3FsYB7CyMu2/+hyx4ToC9hXH7G+QcByetgCvPKIu96JtlcUeVhQGwrTCuSe/taRCr9uy5F+69sCz29gsvL4prsv/bVRj3pGV9OsgJ9n2rNkGuKQst61x4tDAOel9MKrGjQc4m9U53HHe48UiuJEmSquOQK0mSpOo45EqSJKk6DrmSJEmqjkOuJEmSquOQK0mSpOo45EqSJKk6DrmSJEmqjkOuJEmSquOQK0mSpOo45EqSJKk6DrmSJEmqjkOuJEmSqrOkjSfdCryvMHbnQhYypJ8Uxu1okHNvg9jpjuPGxjLg1LLQk75ZFjdRFgbAkQ1iVZcp4MOFsfcVxjXZh00Vxu1qkLNJvaXc585vD+U9+OOFLGRIuzuOA/uoCY/kSpIkqToOuZIkSaqOQ64kSZKq45ArSZKk6sw75EbESRHx9YjYFBF3RMTFXRQmSZIklRpmdYV9wFszc2NEHA1siIivZuadLdcmSZIkFZn3SG5mbs3Mjf3/3wlsAla3XZgkSZJU6pDWyY2ISeB5wM0HuW8tsBbgqIWoTOrAYN/+6ooRFyMdgsHePWbEtUiHYrB3PWKmNg39xbOIWAF8DnhLZv78wPsz8+rMXJOZa5YtZIVSiwb7dpWNq0VksHeXj7oY6RAM9u7TRl2MqjbUkBsRS+kNuNdl5vXtliRJkiQ1M8zqCgF8DNiUmZe1X5IkSZLUzDBHcs8BLgBeHBG39v97ect1SZIkScXm/eJZZn4biA5qkSRJkhaEVzyTJElSdQ5pCbFh7QEeKIydKozbWxgHsLswbn+DnE3qnW4Qq9ntmoLvfqQsdmdhzm2FcVDeQxMNcjZh37ZnD3BfYezmwrgm+7DSz4v73Po8BFxbGFu6/yztP4BdDWJLNem/Jn1fA4/kSpIkqToOuZIkSaqOQ64kSZKq45ArSZKk6jjkSpIkqToOuZIkSaqOQ64kSZKq45ArSZKk6jjkSpIkqToOuZIkSaqOQ64kSZKq45ArSZKk6jjkSpIkqTpL2njSvcADhbE7FrCOYU13HKfD08+A/yyMvbEw7sjCOICdhXH2bX0eA+4qjJ0qjGtyhGR3YZy9W59HgfWFsT8sjFtaGAe9ekvsb5BT5TySK0mSpOo45EqSJKk6DrmSJEmqjkOuJEmSqjP0kBsRExFxS0R8oc2CJEmSpKYO5UjuxcCmtgqRJEmSFspQQ25EnAi8Arim3XIkSZKk5oZdJ/cK4B3A0bM9ICLWAmuh2Rp0UpcG+/aYEdciHYrB3j1ixLVIh2Kwd5eNuBbVbd4juRHxSuDBzNww1+My8+rMXJOZa1q5woTUgsG+dWerxWSwdz2woMVksHefMupiVLVhTlc4B3hVRNwDfAZ4cURc22pVkiRJUgPzDrmZ+c7MPDEzJ4Hzga9l5utbr0ySJEkq5Dq5kiRJqs4hnT6bmd8AvtFKJZIkSdIC8UiuJEmSqtPKQgj7gV2FsVOFcU2+XVy6/M7eBjmnG8SqHbuBOwtjNxbGnVAYB/BoYdzuBjn3N4hVe6aBnYWx2wrjjiyMA5gojHOfW5+9wAOFsZsL444tjIPyPtrTIKf73XIeyZUkSVJ1HHIlSZJUHYdcSZIkVcchV5IkSdVxyJUkSVJ1HHIlSZJUHYdcSZIkVcchV5IkSdVxyJUkSVJ1HHIlSZJUHYdcSZIkVcchV5IkSdVxyJUkSVJ1HHIlSZJUnSVtPOk0sLMwdqIwbm9hXNNY1eMxYHNh7NLCuEcL46D8M9bE9Ahyan7TwI7C2FHsc3c3iFVd9gLbCmNLe7dJ/5X2vfvO0fBIriRJkqrjkCtJkqTqOORKkiSpOkMNuRFxbESsi4jvR8SmiHhh24VJkiRJpYb94tmVwJcz848i4ghgeYs1SZIkSY3MO+RGxDHA7wBvAMjMPcCedsuSJEmSyg1zusIpwBTwiYi4JSKuiYijDnxQRKyNiPURsX7/gpcptWOwb13iRYuJ+1wtVu531ZXIzLkfELEG+C5wTmbeHBFXAj/PzHfPFrM0Io8vLKh03VA/KHXZDUxnRpc5l0XkKYWx9xfGPem3xUPgOrmHp0dhQ2au6TLnERG5qjDWPhKMZp8L8JSI/JXC2J8WxpWuaw6uk3s4mqt3hzmSuwXYkpk392+vA85YoNokSZKkBTfvkJuZ24D7IuLZ/U3nAne2WpUkSZLUwLCrK7wJuK6/ssJm4I3tlSRJkiQ1M9SQm5m3Ap2eYyZJkiSV8opnkiRJqs6wpysckv24SoIWn72Ur5JQuoRTk373s6IZ05SvklDaRxOFcdKgfZSvklDau01WV3C/u7h4JFeSJEnVcciVJElSdRxyJUmSVB2HXEmSJFXHIVeSJEnVcciVJElSdRxyJUmSVB2HXEmSJFXHIVeSJEnVcciVJElSdRxyJUmSVB2HXEmSJFXHIVeSJEnVcciVJElSdZa09cTTbT2x1JIE9hfGlvZ7aT6AoxrElhqXz/VifJ2LsWYJuu/dJvmWLlgVw2vy98S480iuJEmSquOQK0mSpOo45EqSJKk6Qw25EXFJRNwREbdHxKcj4si2C5MkSZJKzTvkRsRq4M3Amsw8DZgAzm+7MEmSJKnUsKcrLAGWRcQSYDnwQHslSZIkSc3MO+Rm5v3Ah4B7ga3Aw5n5lQMfFxFrI2J9RKzPha9TaoV9q8XK3tViZe+qK5E5d4tFxHHA54DXAjuAzwLrMvPa2WImItKTdtXEbmA6M7rMORGRpWvPlq67OIq1bpsYl7VYm7zOh2FDZq5ZsGKGMIp97kSD2HHpo8VkFPtcGE3vjmKt2yZcJ3duu5i9d4c5XeElwN2ZOZWZe4HrgbMXsD5JkiRpQQ0z5N4LnBURyyMigHOBTe2WJUmSJJUb5pzcm4F1wEbgtn7M1S3XJUmSJBVbMsyDMvM9wHtarkWSJElaEF7xTJIkSdUZ6kiuNC66/tb3ro7zaTiL7dvXo+AKCVqs9o66AB1UkxVbZuORXEmSJFXHIVeSJEnVcciVJElSdRxyJUmSVB2HXEmSJFXHIVeSJEnVcciVJElSdRxyJUmSVB2HXEmSJFXHIVeSJEnVcciVJElSdRxyJUmSVB2HXEmSJFXHIVeSJEnVWdLGk+6H7Y/Cj2e5eyWwvY28czDn4sv5zBaec07z9C3U9f6ac3x6t6b39nDLOaq8beTsvG/B3jXngpi1dyMzW8g3u4hYn5lrzGnOxWZc3l9z1mVc3ttR/TzH6bV2bVzeW3O2x9MVJEmSVB2HXEmSJFVnFEPu1eY05yI1Lu+vOesyLu/tqH6e4/RauzYu7605W9L5Obk1i4hHMnPFAdveC/wlMDWw+feA04H/Bu7ub9sO/AA4BzgCOLl/G+B9mbmurbo1XiJiGrgNWArsAz4JXJGZ+/v3vwi4DDimH3JZZl4dEccCPwJWZmZGxAuBG4GTMnNLRDyVXj+vBD4OvBQ4JTMfi4iVwPrMnOzqdao+A727hF6vXZCZOyJiEvhCZp7Wf9yZwD8Aq4GdwFbg0sy8rb9PfiQzPzTwvPcALwBu6G96BjDN4/vtMzNzT7uvTuPuwD7ub3sv8AhwGvC7wMPAfuDCzLxpBGUuKq2srqAnuXxwhwoQEQDfysxXHvjggUY/vZPqNG5+MdNbEfF04N+BpwLviYhn9G+/JjM39ofTGyLi/sz8YkRsA54D3AmcDdzS//M/gLOAmzNzf7+/p4E/Bz7c6atTzQZ795PAhcD7Bx8QEb9Mrx//JDNv7G97EfBr9Abk2UwPPPd7OWAQlg4Db8/MdRHx+8C/AL856oIOd56TK42xzHwQWAtcFL3J9ELgXzNzY//+7cA7gEv7Id+hN9TS//PyA27fOPD0VwCXRIS/TKsNN9E7Unugi4BPzgy4AJn57cz8r64Kk1r2TeDXR13EYuCQ241LIuLW/n9fH9j+2wPb3zWy6jTWMnMzvX3B04HnAhsOeMj6/nboDbEzQ+0pwGeBmSVhzqY3BM+4F/g2cMHCV61xFhETwLnA5w9y93OBjfM8xeA++VbghAUuUWrTHzD3v0qozyMs3XjS6Qp9Bz1dQRqBGPjzYCfqz2z7DnBpRJwM3JOZu6NnBfB84H8PiPsAvUHkiy3UrPGzrD+UTtL7Zeyr8wVExM30zi//SmZe3N98+UHOyZVGbbYvSc1s/8eI+Ft654r/RTclLW4eyZXGXEScQu/82QeBO3j8yOyM59M7B5fM/CFwHL0jCTNfetgAvBG4OzMfGQzMzLuAW4E/bql8jZeZc3KfSe8Luhce5DF3AGfM3MjMFwDvpnfeuXQ4e4je/nXQ03j8KmFvz8zTM/OlmXl7t6UtTg650hiLiFXAR4B/zt5SK1cBb4iI0/v3Hw98kN431WfcBFzM40PuTcBbeOL5uIPeD7xtoWvX+MrMh4E3A2+LiKUH3D3Tw2cPbFveWXFSof5Bgq0RcS5ARDwNOI/eaV8q4OkKC2t5RGwZuH1Z/89LIuL1A9tf011J0pPM/JPvzBJin6Lfq5m5td+rH42Io+mdvnBFZv7PQPx3gJfTO1cXekPuKcwy5GbmHRGxkYGja1JTmXlLRPwfcD7wrYHt2yLitcAHI2I1vX+h2A78/WgqlQ7JnwFXRcQ/9W//XWb+qL9ijQ6R6+RKkiSpOp6uIEmSpOo45EqSJKk6DrmSJEmqjkOuJEmSquOQK0mSpOo45EqSJKk6DrmSJEmqzv8DCZO3MButNTgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# see predicted outcomes for different actions at a particular state\n",
    "state = 0\n",
    "fig,ax = plt.subplots(1, 4, figsize=(12,5), sharex=True, sharey=True)\n",
    "acts = ['LEFT', 'DOWN', 'RIGHT', 'UP']\n",
    "\n",
    "# get the min and max values for plot normalization\n",
    "pmin = 0.0\n",
    "pmax = 0.0\n",
    "for i in range(n_actions):\n",
    "    pred = np.dot(M, states[state])[i]\n",
    "    pmin = min(pred.min(), pmin)\n",
    "    pmax = max(pred.max(), pmax)\n",
    "\n",
    "# do the plot\n",
    "for i in range(n_actions):\n",
    "    pred = np.dot(M, states[state])[i]\n",
    "    ax[i].matshow((pred.reshape((size, size))), vmin=pmin, vmax=pmax, cmap='hot')\n",
    "    ax[i].set_xlabel(acts[i])\n",
    "\n",
    "# print out model-based predictions for state-actions\n",
    "Q = np.dot(np.dot(M, states[state]), rewards)\n",
    "pQ = np.exp(Q*tau)/np.exp(Q*tau).sum()\n",
    "print(*zip(acts, pQ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f8821f70640>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# see what state rewards it's learned\n",
    "plt.matshow(rewards.reshape((size, size)), cmap='bwr_r', vmin=-1, vmax=1.0)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFFFFFFFF\n",
      "FFFFFFFFFF\n",
      "FFFFFFFFFF\n",
      "FFFFFFHHFF\n",
      "FFFFFFFFFF\n",
      "FHFFFFFFFH\n",
      "FFFFFFFHFF\n",
      "FFFFFFFFFF\n",
      "FFFFFFFFFF\n",
      "FFFFFFFFF\u001b[41mG\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 21 timesteps\n"
     ]
    }
   ],
   "source": [
    "# let's see the trained model in action!\n",
    "\n",
    "# reset the to starting point\n",
    "observation, info = env.reset()\n",
    "f0 = states[observation]\n",
    "t0 = states[observation]\n",
    "p_rand = 0.0\n",
    "for t in range(100):\n",
    "    # pick an action following our policy\n",
    "    action = pick_action(f0, t0, M, rewards, tau, p_rand=p_rand)\n",
    "    \n",
    "    # observe the next state\n",
    "    observation, reward, trunc, done, info = env.step(action)\n",
    "    f1 = states[observation]\n",
    "    \n",
    "    # update context\n",
    "    t1 = np.clip(rho*t0 + f1, 0, 1)\n",
    "    \n",
    "    # prepare for next loop\n",
    "    f0 = f1\n",
    "    t0 = t1\n",
    "    \n",
    "    # print it to the screen\n",
    "    clear_output(wait=True)\n",
    "    display(print(env.render()))\n",
    "    \n",
    "    # see if we're done\n",
    "    if trunc:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "    time.sleep(.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SFFFFFFFFF',\n",
       " 'FFFFFFFFFF',\n",
       " 'FFFFFFFFFF',\n",
       " 'FFFFFFHHFF',\n",
       " 'FFFFFFFFFF',\n",
       " 'FHFFFFFFFH',\n",
       " 'FFFFFFFHFF',\n",
       " 'FFFFFFFFFF',\n",
       " 'FFFFFFFFFF',\n",
       " 'FFFFFFFFFG']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFFFF\n",
      "FFFFFFFFFF\n",
      "FFFFFFFFFF\n",
      "FFFFFFHHFF\n",
      "FFFFFFFFFG\n",
      "FHFFFFFFFH\n",
      "FFFFFFFHFF\n",
      "FFFFFFFFFF\n",
      "FFFFFFFFFF\n",
      "FFFFFFFFFH\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remap the goal location\n",
    "new_desc = desc[:]\n",
    "drow = list(new_desc[-1])\n",
    "drow[-1] = 'H'\n",
    "new_desc[-1] = ''.join(drow)\n",
    "drow = list(new_desc[4])\n",
    "drow[-1] = 'G'\n",
    "new_desc[4] = ''.join(drow)\n",
    "new_desc\n",
    "env = frozen_lake.FrozenLakeEnv(desc=new_desc,\n",
    "                                is_slippery=slippery,\n",
    "                                render_mode='ansi')\n",
    "env.reset()\n",
    "display(print(env.render()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Booyah!!!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
